# üöÄ Pipeline Int√©gr√© de G√©n√©ration de Leads

Un outil puissant qui automatise toute la cha√Æne de g√©n√©ration de leads en **une seule commande**.

## ‚ú® Ce que fait le pipeline

```
URL ‚Üí Scraper ‚Üí Noms d'entreprises ‚Üí Domain Finder ‚Üí Domaines ‚Üí Enricher ‚Üí Donn√©es de contact
```

**Tout se passe en m√©moire** : pas de fichiers CSV interm√©diaires √† g√©rer !

---

## üéØ Utilisation

### 1Ô∏è‚É£ En ligne de commande (Simple et rapide)

```bash
# Exemple basique
python3 lead_pipeline.py https://example.com/exhibitors

# Avec options
python3 lead_pipeline.py https://example.com/exhibitors \
  --max-pages 10 \
  --output output/mes_leads

# Voir toutes les options
python3 lead_pipeline.py --help
```

**R√©sultat :**
- `output/mes_leads.csv` - Format tableur
- `output/mes_leads.xlsx` - Excel
- `output/mes_leads.json` - JSON complet avec toutes les donn√©es

---

### 2Ô∏è‚É£ Via l'interface web

```bash
# D√©marrer le serveur
python3 app.py
```

Puis ouvrez : `http://localhost:5000`

**Deux modes disponibles :**

#### Mode Pipeline Complet (Nouveau !)
1. Collez l'URL du site d'exposants
2. Cliquez sur "Run Full Pipeline"
3. Attendez que tout se termine
4. T√©l√©chargez vos r√©sultats

#### Mode √âtape par √âtape (Pour plus de contr√¥le)
1. **Scrape** ‚Üí R√©cup√®re les noms d'entreprises
2. **Find Domains** ‚Üí Trouve les domaines
3. **Enrich** ‚Üí Enrichit avec emails, t√©l√©phones, LinkedIn
4. **Export** ‚Üí T√©l√©charge aux formats CSV/Excel/JSON

---

## üìä Ce que vous obtenez

Pour chaque entreprise trouv√©e :

| Colonne | Description |
|---------|-------------|
| `company_name` | Nom de l'entreprise |
| `domain` | Site web (ex: `exemple.fr`) |
| `email` | Email de contact |
| `phone` | T√©l√©phone |
| `linkedin` | Page LinkedIn entreprise |
| `address` | Adresse (si disponible) |
| `city` | Ville |
| `siren` | N¬∞ SIREN (entreprises fran√ßaises) |
| `siret` | N¬∞ SIRET (entreprises fran√ßaises) |
| `executive_first_name` | Pr√©nom du dirigeant |
| `executive_last_name` | Nom du dirigeant |
| `executive_role` | Fonction |
| `executive_email` | Email du dirigeant |
| `executive_linkedin` | LinkedIn du dirigeant |
| `data_sources` | Sources des donn√©es |

---

## üîß Configuration (Optionnel)

Pour de meilleurs r√©sultats, ajoutez des cl√©s API **gratuites** :

### 1. Pappers (pour les entreprises fran√ßaises)
- Inscrivez-vous : https://www.pappers.fr/api
- 10 000 requ√™tes/mois **GRATUITES**
- Donne : SIREN, SIRET, dirigeants, adresses

### 2. Hunter.io (pour les emails)
- Inscrivez-vous : https://hunter.io/users/sign_up
- 50 requ√™tes/mois **GRATUITES**
- Donne : emails des dirigeants

### Configuration

Cr√©ez un fichier `.env` :

```bash
PAPPERS_API_KEY=votre_cl√©_pappers
HUNTER_API_KEY=votre_cl√©_hunter
```

**Sans cl√©s API :** Le syst√®me fonctionne quand m√™me en scrappant les sites web directement.

---

## üí° Exemples d'utilisation

### Exemple 1 : Salon Equipauto
```bash
python3 lead_pipeline.py "https://new-liste-exposants.hubj2c.com/" --max-pages 10
```

### Exemple 2 : N'importe quel salon/annuaire
```bash
python3 lead_pipeline.py "https://votre-salon.com/exposants" --max-pages 5
```

### Exemple 3 : En Python (pour int√©gration)
```python
from lead_pipeline import LeadPipeline

# Cr√©er le pipeline
pipeline = LeadPipeline()

# Ex√©cuter
results = pipeline.run(
    url="https://example.com/exhibitors",
    max_pages=10,
    export_csv=True,
    output_prefix="output/my_leads"
)

# Acc√©der aux r√©sultats
print(f"Entreprises trouv√©es : {results['stats']['total_companies_scraped']}")
print(f"Domaines trouv√©s : {results['stats']['domains_found']}")
print(f"Emails trouv√©s : {results['stats']['emails_found']}")

# Les donn√©es enrichies
for company in results['companies_enriched']:
    print(f"{company['company_name']} - {company['company_email']}")
```

---

## üé® Architecture

Le pipeline utilise **3 modules optimis√©s** :

### 1. `universal_scraper.py`
- Scrape n'importe quel site web
- D√©tection automatique des entreprises
- Gestion automatique de la pagination
- Pas besoin de code sp√©cifique par site

### 2. `domain_finder.py`
- Trouve les domaines via Clearbit (gratuit)
- Validation intelligente (d√©tecte les domaines park√©s)
- Score de confiance pour chaque domaine
- Taux de faux positifs estim√©

### 3. `company_enricher.py`
- Scrape les pages de contact
- APIs Pappers + Hunter (optionnelles)
- Emails, t√©l√©phones, LinkedIn
- Donn√©es des dirigeants

### 4. `lead_pipeline.py` ‚≠ê (NOUVEAU)
- **Orchestre les 3 modules**
- Tout en m√©moire (rapide)
- Export optionnel √† la fin
- Progress tracking en temps r√©el

---

## ‚ö° Performance

| M√©trique | Valeur |
|----------|--------|
| Vitesse | ~3-5 secondes par entreprise |
| Taux de succ√®s domaines | 60-80% |
| Taux de succ√®s emails | 40-60% |
| Taux de succ√®s t√©l√©phones | 30-50% |

**Exemple pour 100 entreprises :**
- Temps total : ~10-15 minutes
- Domaines trouv√©s : ~60-80
- Emails trouv√©s : ~30-50

---

## üö® Limites et bonnes pratiques

### Limites
- Fonctionne mieux avec des sites structur√©s (salons, annuaires)
- N√©cessite une connexion internet stable
- Les cl√©s API gratuites ont des quotas

### Bonnes pratiques
1. **Commencez petit** : Testez avec `--max-pages 2` d'abord
2. **V√©rifiez les r√©sultats** : Les domaines avec confiance < 70% peuvent √™tre faux
3. **Respectez les quotas** : Pappers = 10k/mois, Hunter = 50/mois
4. **Soyez patient** : Le scraping prend du temps pour √™tre respectueux

---

## üêõ D√©pannage

### Probl√®me : "No companies found"
- V√©rifiez que l'URL contient bien une liste d'entreprises
- Le site peut utiliser du JavaScript lourd (le scraper g√®re Selenium)

### Probl√®me : "Peu de domaines trouv√©s"
- Normal ! Certaines entreprises n'ont pas de site web
- Essayez d'ajouter la cl√© Pappers pour les entreprises fran√ßaises

### Probl√®me : "Peu d'emails trouv√©s"
- Normal aussi ! Beaucoup de sites cachent les emails
- Ajoutez la cl√© Hunter pour am√©liorer

### Probl√®me : Le scraper est lent
- C'est normal, il faut √™tre respectueux des serveurs
- On attend 1-2 secondes entre chaque requ√™te

---

## üìù Fichiers du projet

```
web-scraper/
‚îú‚îÄ‚îÄ lead_pipeline.py          ‚≠ê Pipeline int√©gr√© (NOUVEAU)
‚îú‚îÄ‚îÄ universal_scraper.py      üåê Scraper universel
‚îú‚îÄ‚îÄ domain_finder.py          üîç Chercheur de domaines
‚îú‚îÄ‚îÄ company_enricher.py       üíº Enrichisseur de donn√©es
‚îú‚îÄ‚îÄ app.py                    üñ•Ô∏è  Interface web
‚îú‚îÄ‚îÄ requirements.txt          üì¶ D√©pendances
‚îú‚îÄ‚îÄ .env.example              üîê Exemple de config
‚îî‚îÄ‚îÄ output/                   üìÅ R√©sultats export√©s
```

---

## üéÅ Avantages vs l'ancienne m√©thode

| Ancienne m√©thode | Nouvelle m√©thode (Pipeline) |
|------------------|----------------------------|
| 3 commandes s√©par√©es | 1 seule commande |
| Fichiers CSV interm√©diaires | Tout en m√©moire |
| Import/Export manuel | Automatique |
| Configuration par site | Universel |
| ~20 minutes de manip | ~2 minutes de manip |

---

## ü§ù Contribution

Ce pipeline est modulaire :
- Modifiez `universal_scraper.py` pour am√©liorer la d√©tection
- Modifiez `domain_finder.py` pour ajouter des sources
- Modifiez `company_enricher.py` pour plus de donn√©es

Chaque module fonctionne ind√©pendamment ET ensemble !

---

## üìû Support

Questions ? Probl√®mes ?
- Consultez les logs d√©taill√©s dans le terminal
- V√©rifiez le fichier `.env` pour les cl√©s API
- Testez chaque module individuellement pour d√©bugger

---

**Fait avec ‚ù§Ô∏è pour simplifier la g√©n√©ration de leads**
